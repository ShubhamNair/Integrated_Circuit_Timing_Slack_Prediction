# -*- coding: utf-8 -*-
"""ShubhamNair_DS_Final_coding.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x4oh4Wng3T7fPmg9gdNhQaF-xhL7I17I
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
import xgboost
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from lightgbm import LGBMRegressor
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
import pandas as pd
import torch
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt

pip install category_encoders

pip install optuna

#Formatting and loading the data
labels_headers=["TimingSlack_Optimized","Condition","Timing_Endpoint"]
features_headers=["TimingSlack_PreOpt"]
for i in range(31):
  features_headers.append("Feature_"+f"{i+1}")

features_headers.append("Condition")
features_headers.append("Timing_Endpoint")

labels_df = pd.read_csv("/content/drive/MyDrive/DataScience_Final/mod_labels.csv", sep=r'\s+', header=None, names=labels_headers)
features_df = pd.read_csv("/content/drive/MyDrive/DataScience_Final/mod_features.csv", delim_whitespace=True, header=None,names=features_headers)

merged_df = pd.merge(labels_df, features_df, on=["Condition", "Timing_Endpoint"])

merged_df.info()

# Drop 'Condition' and 'Timing_Endpoint' columns from merged_df
merged_df = merged_df.drop(['Condition', 'Timing_Endpoint'], axis=1)
merged_df['Feature_1'] = merged_df['Feature_1'].map({'na': 0, 'IO': 1})

# Filter data based on specified slack range and conditions
filtered_df = merged_df[(merged_df['TimingSlack_Optimized'] >= -0.5) & (merged_df['TimingSlack_Optimized'] <= 0.3)]

filter_header=filtered_df.columns.tolist()

filtered_df.head()

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(filtered_df.drop(['TimingSlack_Optimized'], axis=1), filtered_df['TimingSlack_Optimized'], test_size=0.2)

def custom_asymmetric_loss_updated(y_true, y_pred):
    residual = y_true - y_pred
    loss = np.where(residual < 0, 50.0 * residual**2, residual**2)
    return np.mean(loss)

def objective_updated(trial):
    #Hyperparameter search space
    xgb_params = {
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'gamma': trial.suggest_float('gamma', 0, 0.5),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0)
    }

    #XGBoost regressor
    xgb_model = XGBRegressor(**xgb_params, objective='reg:squarederror', eval_metric='rmse', scale_pos_weight=1)

    # Train the model
    weights = [50.0 if y < 0 else 1.0 for y in y_train]
    xgb_model.fit(X_train, y_train, sample_weight=weights)

    # Predictions
    y_pred = xgb_model.predict(X_test)

    # Evaluate the model
    rmse = mean_squared_error(y_test, y_pred, squared=False)

    # Penalty for positive slack values
    positive_slack_penalty = 0.5  # Adjust as needed
    y_pred_positive_penalty = [pred + positive_slack_penalty if pred >= 0 else pred for pred in y_pred]
    rmse_penalty = mean_squared_error(y_test, y_pred_positive_penalty, squared=False)

    # Combine both penalties (negative slack is already emphasized by the sample weights)
    combined_rmse = rmse + rmse_penalty

    return combined_rmse

import optuna
# Run Optuna optimization
optuna_study = optuna.create_study(direction='minimize')
optuna_study.optimize(objective_updated, n_trials=20, n_jobs=-1)

# Get the best hyperparameters
best_xgb_params_optuna = optuna_study.best_params

# Train the model with the best hyperparameters
best_xgb_model_optuna_tuned = XGBRegressor(**best_xgb_params_optuna, objective='reg:squarederror', eval_metric='rmse', scale_pos_weight=1)
best_xgb_model_optuna_tuned.fit(X_train, y_train, sample_weight=[50.0 if y < 0 else 1.0 for y in y_train])

# Predictions
y_pred_optuna_tuned = best_xgb_model_optuna_tuned.predict(X_test)

# Evaluate the model
rmse_optuna_tuned = mean_squared_error(y_test, y_pred_optuna_tuned, squared=False)
r2_optuna_tuned = r2_score(y_test, y_pred_optuna_tuned)
print('Tuned XGBoost Model (Optuna):')
print('Best Hyperparameters:', best_xgb_params_optuna)
print('RMSE:', rmse_optuna_tuned)

# Calculate the RMSE for negative slack 0ns to -0.5ns
negative_slack_optuna = (y_test <= 0) & (y_test >= -0.5)
y_test_neg_optuna = y_test[negative_slack_optuna]
y_pred_neg_optuna_tuned = y_pred_optuna_tuned[negative_slack_optuna]
rmse_neg_optuna_tuned = mean_squared_error(y_test_neg_optuna, y_pred_neg_optuna_tuned, squared=False)
print('RMSE for negative slack 0ns to -0.5ns:', rmse_neg_optuna_tuned)

# Correlation plot
plt.scatter(y_test, y_pred_optuna_tuned)
plt.plot([-0.5, 0.3], [-0.5, 0.3], color='red')
plt.xlabel('Ground Truth')
plt.ylabel('Predicted Label (Tuned XGBoost - Optuna)')
plt.show()

# Define a custom asymmetric objective function
def custom_asymmetric_loss_objective(y_true, y_pred):
    residual = y_true - y_pred
    grad = np.where(residual < 0, -50.0 * residual, -residual)
    hess = np.where(residual < 0, 50.0, 1.0)
    return grad, hess

# Define the objective function for Optuna optimization
def objective_lightgbm(trial):
    params_lightgbm = {
        'objective': custom_asymmetric_loss_objective,
        'boosting_type': 'gbdt',
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),
        'num_leaves': trial.suggest_int('num_leaves', 20, 200),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'min_child_weight': trial.suggest_float('min_child_weight', 0.1, 10),
        'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 10),
        'reg_alpha': trial.suggest_float('reg_alpha', 0.1, 10)
    }

    model_lgbm = LGBMRegressor(**params_lightgbm)
    model_lgbm.fit(X_train, y_train)
    y_pred_lgbm = model_lgbm.predict(X_test)

    rmse_lgbm = mean_squared_error(y_test, y_pred_lgbm, squared=False)
    return rmse_lgbm

# Run Optuna optimization for LightGBM
study_lightgbm = optuna.create_study(direction='minimize')
study_lightgbm.optimize(objective_lightgbm, n_trials=20, n_jobs=-1)

# Get the best hyperparameters for LightGBM
best_params_lightgbm = study_lightgbm.best_params

# Train the LightGBM model with the best hyperparameters
best_model_lgbm = LGBMRegressor(**best_params_lightgbm, objective=custom_asymmetric_loss_objective)
best_model_lgbm.fit(X_train, y_train)
y_pred_lgbm_tuned = best_model_lgbm.predict(X_test)

# Evaluate the LightGBM model
rmse_lgbm_tuned = mean_squared_error(y_test, y_pred_lgbm_tuned, squared=False)
r2_lgbm_tuned = r2_score(y_test, y_pred_lgbm_tuned)

# Calculate the RMSE for negative slack 0ns to -0.5ns
y_test_neg_lgbm = y_test[negative_slack_optuna]
y_pred_neg_lgbm_tuned = y_pred_lgbm_tuned[negative_slack_optuna]
rmse_neg_lgbm_tuned = mean_squared_error(y_test_neg_lgbm, y_pred_neg_lgbm_tuned, squared=False)

# Display results for LightGBM
print('Tuned LightGBM Model (Optuna):')
print('Best Hyperparameters:', best_params_lightgbm)
print('RMSE:', rmse_lgbm_tuned)
print('R2:', r2_lgbm_tuned)
print('RMSE for negative slack 0ns to -0.5ns:', rmse_neg_lgbm_tuned)

# Correlation plot for LightGBM
plt.scatter(y_test, y_pred_lgbm_tuned)
plt.plot([-0.5, 0.3], [-0.5, 0.3], color='red')
plt.xlabel('Ground Truth')
plt.ylabel('Predicted Label (Tuned LightGBM - Optuna)')
plt.show()

# Define a custom neural network model
class CustomNN(nn.Module):
    def __init__(self, input_size):
        super(CustomNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Create PyTorch tensors for the dataset
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

# Create a custom dataset and data loader
custom_dataset = TensorDataset(X_train_tensor, y_train_tensor)
custom_data_loader = DataLoader(custom_dataset, batch_size=64, shuffle=True)

# Custom loss function with different weight for negative slack values
class CustomMSELoss(nn.Module):
    def __init__(self, negative_weight):
        super(CustomMSELoss, self).__init__()
        self.negative_weight = negative_weight

    def forward(self, predictions, targets):
        difference = predictions - targets
        weights = torch.ones_like(targets)
        weights[targets < 0] = self.negative_weight
        weighted_sq_diff = weights * (difference ** 2)
        loss = weighted_sq_diff.mean()
        return loss

# Create the custom neural network model
custom_model = CustomNN(input_size=X_train.shape[1])
custom_optimizer = torch.optim.Adam(custom_model.parameters(), lr=0.001)
num_epochs = 7
custom_losses = []

negative_weight_custom = 50
custom_criterion = CustomMSELoss(negative_weight=negative_weight_custom)

# Training Loop for the custom model
for epoch in range(num_epochs):
    epoch_losses_custom = []
    for inputs, labels in custom_data_loader:
        custom_optimizer.zero_grad()
        custom_outputs = custom_model(inputs)
        custom_loss = custom_criterion(custom_outputs, labels.unsqueeze(1))
        custom_loss.backward()
        custom_optimizer.step()
        epoch_losses_custom.append(custom_loss.item())

    print(f"Epoch {epoch+1}/{num_epochs}, Loss: {custom_loss.item()}")
    epoch_loss_custom = np.mean(epoch_losses_custom)
    custom_losses.append(epoch_loss_custom)

# Evaluate the custom model
custom_model.eval()
with torch.no_grad():
    custom_predictions = custom_model(X_test_tensor).squeeze()
    custom_predictions = custom_predictions.numpy()
    y_test_custom_np = y_test_tensor.numpy()

custom_rmse = np.sqrt(mean_squared_error(y_test_custom_np, custom_predictions))
print(f"Custom RMSE: {custom_rmse}")

mask_custom = (y_test_custom_np >= -0.5) & (y_test_custom_np <= 0)
filtered_y_test_custom = y_test_custom_np[mask_custom]
filtered_predictions_custom = custom_predictions[mask_custom]

# Calculate RMSE for the filtered data
filtered_rmse_custom = np.sqrt(mean_squared_error(filtered_y_test_custom, filtered_predictions_custom))
print(f"Filtered RMSE (for values between -0.5 and 0): {filtered_rmse_custom}")

# Correlation plot for the custom model
plt.scatter(y_test_custom_np, custom_predictions)
plt.plot([-0.5, 0.3], [-0.5, 0.3], color='red')
plt.xlabel('Ground Truth')
plt.ylabel('Predicted Label (Custom Model)')
plt.show()

pip install catboost

#Extra Credit
from catboost import CatBoostRegressor

cat_model = CatBoostRegressor(
    learning_rate=0.01,
    n_estimators=1000,
    max_depth=10,
    l2_leaf_reg=1,
    loss_function='RMSE',
    eval_metric='RMSE',
    cat_features=['Feature_1']
)


weights = [50.0 if y < 0 else 1.0 for y in y_train]

cat_model.fit(X_train, y_train, sample_weight=weights)


y_pred_cat = cat_model.predict(X_test)


rmse_cat = mean_squared_error(y_test, y_pred_cat, squared=False)
r2_cat = r2_score(y_test, y_pred_cat)
print('RMSE:', rmse_cat)
print('R2:', r2_cat)


negative_slack = (y_test <= 0) & (y_test >= -0.5)
y_test_neg = y_test[negative_slack]
y_pred_neg_cat = y_pred_cat[negative_slack]
rmse_neg_cat = mean_squared_error(y_test_neg, y_pred_neg_cat, squared=False)
print('RMSE for negative slack 0ns to -0.5ns:', rmse_neg_cat)


plt.scatter(y_test, y_pred_cat)
plt.plot([-0.5, 0.3], [-0.5, 0.3], color='red')
plt.xlabel('Ground Truth')
plt.ylabel('Predicted Label')
plt.show()